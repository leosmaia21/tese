"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from contextlib import contextmanager

logger = ...
@contextmanager
def torch_distributed_zero_first(local_rank: int): # -> Generator[None, None, None]:
    """
    Decorator to make all processes in distributed training wait for each local_master to do something.
    """
    ...

def init_torch_seeds(seed=...): # -> None:
    ...

def date_modified(path=...): # -> str:
    ...

def git_describe(path=...): # -> str:
    ...

def select_device(device=..., batch_size=...): # -> device:
    ...

def time_synchronized(): # -> float:
    ...

def profile(x, ops, n=..., device=...): # -> None:
    ...

def is_parallel(model): # -> bool:
    ...

def intersect_dicts(da, db, exclude=...): # -> dict[Unknown, Unknown]:
    ...

def initialize_weights(model): # -> None:
    ...

def find_modules(model, mclass=...): # -> list[int]:
    ...

def sparsity(model): # -> float:
    ...

def prune(model, amount=...): # -> None:
    ...

def fuse_conv_and_bn(conv, bn): # -> Conv2d:
    ...

def model_info(model, verbose=..., img_size=...): # -> None:
    ...

def load_classifier(name=..., n=...): # -> Any:
    ...

def scale_img(img, ratio=..., same_shape=..., gs=...): # -> Tensor:
    ...

def copy_attr(a, b, include=..., exclude=...): # -> None:
    ...

class ModelEMA:
    """ Model Exponential Moving Average from https://github.com/rwightman/pytorch-image-models
    Keep a moving average of everything in the model state_dict (parameters and buffers).
    This is intended to allow functionality like
    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage
    A smoothed version of the weights is necessary for some training schemes to perform well.
    This class is sensitive where it is initialized in the sequence of model init,
    GPU assignment and distributed training wrappers.
    """
    def __init__(self, model, decay=..., updates=...) -> None:
        ...
    
    def update(self, model): # -> None:
        ...
    
    def update_attr(self, model, include=..., exclude=...): # -> None:
        ...
    


class BatchNormXd(torch.nn.modules.batchnorm._BatchNorm):
    ...


def revert_sync_batchnorm(module): # -> BatchNormXd:
    ...

class TracedModel(nn.Module):
    def __init__(self, model=..., device=..., img_size=...) -> None:
        ...
    
    def forward(self, x, augment=..., profile=...):
        ...
    


