"""
This type stub file was generated by pyright.
"""

import torch
from torch.utils.data import Dataset

help_url = ...
img_formats = ...
vid_formats = ...
logger = ...
def get_hash(files): # -> int:
    ...

def exif_size(img): # -> tuple[Unknown, Unknown]:
    ...

def create_dataloader(path, imgsz, batch_size, stride, opt, hyp=..., augment=..., cache=..., pad=..., rect=..., rank=..., world_size=..., workers=..., image_weights=..., quad=..., prefix=...): # -> tuple[Unknown | InfiniteDataLoader, LoadImagesAndLabels]:
    ...

class InfiniteDataLoader(torch.utils.data.dataloader.DataLoader):
    """ Dataloader that reuses workers

    Uses same syntax as vanilla DataLoader
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def __iter__(self): # -> Generator[Unknown, None, None]:
        ...
    


class _RepeatSampler:
    """ Sampler that repeats forever

    Args:
        sampler (Sampler)
    """
    def __init__(self, sampler) -> None:
        ...
    
    def __iter__(self): # -> Generator[Unknown, None, None]:
        ...
    


class LoadImages:
    def __init__(self, path, img_size=..., stride=...) -> None:
        ...
    
    def __iter__(self): # -> Self@LoadImages:
        ...
    
    def __next__(self): # -> tuple[str, NDArray[Unknown], Unknown, Unknown | None]:
        ...
    
    def new_video(self, path): # -> None:
        ...
    
    def __len__(self): # -> int:
        ...
    


class LoadWebcam:
    def __init__(self, pipe=..., img_size=..., stride=...) -> None:
        ...
    
    def __iter__(self): # -> Self@LoadWebcam:
        ...
    
    def __next__(self): # -> tuple[Literal['webcam.jpg'], NDArray[Unknown], Unknown, None]:
        ...
    
    def __len__(self): # -> Literal[0]:
        ...
    


class LoadStreams:
    def __init__(self, sources=..., img_size=..., stride=...) -> None:
        ...
    
    def update(self, index, cap): # -> None:
        ...
    
    def __iter__(self): # -> Self@LoadStreams:
        ...
    
    def __next__(self): # -> tuple[list[str], NDArray[Unknown], list[None], None]:
        ...
    
    def __len__(self): # -> Literal[0]:
        ...
    


def img2label_paths(img_paths): # -> list[str]:
    ...

class LoadImagesAndLabels(Dataset):
    def __init__(self, path, img_size=..., batch_size=..., augment=..., hyp=..., rect=..., image_weights=..., cache_images=..., single_cls=..., stride=..., pad=..., prefix=...) -> None:
        ...
    
    def cache_labels(self, path=..., prefix=...):
        ...
    
    def __len__(self): # -> int:
        ...
    
    def __getitem__(self, index): # -> tuple[Tensor, Tensor, Unknown | Any, tuple[tuple[Unknown, Unknown], tuple[tuple[Unknown, Unknown], tuple[Any | float | Unknown, Any | float | Unknown]]] | None]:
        ...
    
    @staticmethod
    def collate_fn(batch): # -> tuple[Tensor, Tensor, tuple[Unknown], tuple[Unknown]]:
        ...
    
    @staticmethod
    def collate_fn4(batch): # -> tuple[Tensor, Tensor, tuple[Unknown, ...], tuple[Unknown, ...]]:
        ...
    


def load_image(self, index): # -> tuple[Unknown, tuple[Unknown, Unknown], Unknown] | tuple[Unknown, Unknown, Unknown]:
    ...

def augment_hsv(img, hgain=..., sgain=..., vgain=...): # -> None:
    ...

def hist_equalize(img, clahe=..., bgr=...):
    ...

def load_mosaic(self, index): # -> tuple[Unknown, Unknown]:
    ...

def load_mosaic9(self, index): # -> tuple[Unknown, Unknown]:
    ...

def load_samples(self, index): # -> tuple[list[Unknown], list[Unknown], list[Unknown]]:
    ...

def copy_paste(img, labels, segments, probability=...): # -> tuple[Unknown, Unknown | NDArray[Unknown], Unknown]:
    ...

def remove_background(img, labels, segments): # -> tuple[NDArray[signedinteger[Any]], Unknown, Unknown]:
    ...

def sample_segments(img, labels, segments, probability=...): # -> tuple[list[Unknown], list[Unknown], list[Unknown]]:
    ...

def replicate(img, labels): # -> tuple[Unknown, Unknown | NDArray[Any]]:
    ...

def letterbox(img, new_shape=..., color=..., auto=..., scaleFill=..., scaleup=..., stride=...): # -> tuple[Unknown, tuple[float | Unknown, float | Unknown] | tuple[Unknown, Unknown], tuple[Any | float | Unknown, Any | float | Unknown]]:
    ...

def random_perspective(img, targets=..., segments=..., degrees=..., translate=..., scale=..., shear=..., perspective=..., border=...): # -> tuple[Unknown, Unknown]:
    ...

def box_candidates(box1, box2, wh_thr=..., ar_thr=..., area_thr=..., eps=...):
    ...

def bbox_ioa(box1, box2):
    ...

def cutout(image, labels):
    ...

def pastein(image, labels, sample_labels, sample_images, sample_masks): # -> NDArray[Unknown]:
    ...

class Albumentations:
    def __init__(self) -> None:
        ...
    
    def __call__(self, im, labels, p=...): # -> tuple[Unknown, NDArray[Unknown] | Unknown]:
        ...
    


def create_folder(path=...): # -> None:
    ...

def flatten_recursive(path=...): # -> None:
    ...

def extract_boxes(path=...): # -> None:
    ...

def autosplit(path=..., weights=..., annotated_only=...): # -> None:
    """ Autosplit a dataset into train/val/test splits and save path/autosplit_*.txt files
    Usage: from utils.datasets import *; autosplit('../coco')
    Arguments
        path:           Path to images directory
        weights:        Train, val, test weights (list)
        annotated_only: Only use images with an annotated txt file
    """
    ...

def load_segmentations(self, index):
    ...

